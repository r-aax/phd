% OpenMP для разных стратегий распараллеливания для Римановского решателя.
\subsection{Сравнение стратегий распараллеливания векторизованного римановского решателя с помощью OpenMP для микропроцессора Intel Xeon Phi KNL}

Микропроцессоры Intel Xeon Phi KNL 7290, для которых проводилось исследование по распараллеливанию, содержат по 72 ядра, в каждом из которых возможно запустить  по 4 потока, что дает суммарно 288 потоков для одного процессора.
Ввиду этого применение распараллеливания с помощью OpenMP является ожидаемым, так как способно существенно ускорить исполняемый код. Были проанализированы 3 стратегии распараллеливания (см. рис. 2), описание которых приведено ниже.

\begin{figure}[ht]
\centering
\includegraphics[width=1.0\textwidth]{./pics/text_3_omp1/modes.pdf}
\captionstyle{center}\caption{Иллюстрация работы трех рассматриваемых стратегий распараллеливания вычислений: CHUNKS, INTERLEAVE, RACE.}
\label{fig:text_3_omp1_modes}
\end{figure}

В качестве первой стратегии распараллеливания была использована стратегия CHUNKS, в которой массивы входных данных были разделены на равные непрерывные части по числу используемых потоков, каждый поток обрабатывал свои участки массивов входных данных (на листинге ниже nt -- общее количество потоков, \texttt{solver\_16} -- указатель на решатель для обработки 16 экземпляров задачи Римана).

\begin{lstlisting}[caption={caption},label={label}]
// [15], riemann.cpp, 585-599
#pragma omp parallel
{
    int tn = omp_get_thread_num();
    int lb = (int)((c / FP16_VECTOR_SIZE) * ((double)tn / (double)nt));
    int ub = (int)((c / FP16_VECTOR_SIZE) * ((double)(tn + 1) / (double)nt));

    for (int i = lb * FP16_VECTOR_SIZE;
         i < ub * FP16_VECTOR_SIZE;
         i += FP16_VECTOR_SIZE)
    {
        solver_16(dl + i, ul + i, vl + i, wl + i, pl + i,
                  dr + i, ur + i, vr + i, wr + i, pr + i,
                  d + i, u + i, v + i, w + i, p + i);
    }
}
\end{lstlisting}

Во второй стратегии -- INTERLEAVE -- массивы входных данных были разделены на участки по 16 элементов и распределялись между потоками в шахматном порядке (на листинге ниже \texttt{c\_base} -- длина массивов входных данных без учета эпилога цикла, nt и \texttt{solver\_16} имеют тот же смысл, что и в листинге выше).

\begin{lstlisting}[caption={caption},label={label}]
// [15], riemann.cpp, 603-615
#pragma omp parallel
{
    int tn = omp_get_thread_num();

    for (int i = tn * FP16_VECTOR_SIZE;
         i < c_base;
         i += nt * FP16_VECTOR_SIZE)
    {
        solver_16(dl + i, ul + i, vl + i, wl + i, pl + i,
                  dr + i, ur + i, vr + i, wr + i, pr + i,
                  d + i, u + i, v + i, w + i, p + i);
    }
}
\end{lstlisting}

Третья стратегия -- RACE -- основана на ведении глобального адреса следующего готового к обработке участка входных данных.
Как только очередной поток освобождается, он приступает к обработке следующих свободных 16 экземпляров задачи. Таким образом была предпринята попытка избавиться от простоев потоков в случае разного времени выполнения отдельных экземпляров задачи.

\begin{lstlisting}[caption={caption},label={label}]
// [15], riemann.cpp, 620-655
int g = 0;
#pragma omp parallel
{
    int i = 0;
    bool is_break = false;

    while (true)
    {
        #pragma omp critical
        {
            if (g >= c_base)
            {
                is_break = true;
            }
            else
            {
                i = g;
                g += FP16_VECTOR_SIZE;
            }
        }

        if (is_break)
        {
            break;
        }

        solver_16(dl + i, ul + i, vl + i, wl + i, pl + i,
                  dr + i, ur + i, vr + i, wr + i, pr + i,
                  d + i, u + i, v + i, w + i, p + i);
    }
} 
\end{lstlisting}

На данном листинге g -- глобальный счетчик следующей свободной партии экземпляров задачи Римана, доступный всем потокам.
Для его проверки и продвижения требуется блокировка.
Для всех описанных стратегий были выполнены тестовые расчеты на суперкомпьютере МВС-10П.

Тестирование различных стратегий распараллеливания обработки задачи Римана выполнялось на пакете [15].

Единственным параметров теста является максимальное количество потоков, на которых нужно выполнять прогон.
При этом сначала тесты прогоняются на невекторизованном решателе с использованием одного потока (относительно этого запуска считается суммарное ускорение решателя).

После этого выполняется запуск векторизованного решателя для различного количества потоков, начиная от 1 и заканчивая 288.
По результатам прогона выдается время работы как невекторизованного решателя на одном потоке, так и векторизованной версии для разного количества потоков.
Отдельно выполнялись прогоны для стратегий распараллеливания CHUNKS, INTERELEAVE и RACE.
По результатам прогонов были построены графики суммарного ускорения векторизованного и распараллеленного римановского решателя для всех трех стратегий, данные графики представлены на рис. 3.

\begin{figure}[ht]
\centering
\includegraphics[width=1.0\textwidth]{./pics/text_3_omp1/main_graph.png}
\captionstyle{center}\caption{График ускорения распараллеленного векторизованного римановского решателя по сравнению с нераспараллеленной невекторизованной версией.}
\label{fig:text_3_omp1_main_graph}
\end{figure}

Из рис. 3 видно, что стратегия распараллеливания RACE является нежизнеспособной даже на сравнительно небольшом числе потоков.
Блокировка глобального ресурса (счетчик следующей свободной партии задач), оказывается фатальной и приводит к простою потоков.

Эффективность двух других стратегий примерно одинакова, однако стратегия INTERLEAVE показала себя все же лучше, продемонстрировав максимальное ускорение 368 раз на 139 потоках.

На графиках явно просматриваются 4 характерные участка, длина которых совпадает с количеством ядер микропроцессора Intel Xeon Phi KNL 7290 (72 ядра).
На первом участке масштабируемость распараллеливания близка к линейной. На втором участке наблюдается сначала некоторое снижение эффективности, но затем при дальнейшем увеличении количества потоков удается добиться дополнительного ускорения.
На третьем и четвертом участках графиков наблюдается снижение производительности.
Это ожидаемо, так как каждое ядро микропроцессора содержит два векторных исполнительных устройства и при запуске на ядре трех или четырех потоков начинается конкуренция за данные исполнительные устройства.
